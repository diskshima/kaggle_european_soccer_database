{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import tree\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 999)\n",
    "\n",
    "if 'original_figure_figsize' not in locals():\n",
    "    original_figure_figsize = plt.rcParams['figure.figsize']  # Default is [6.0, 4.0]\n",
    "\n",
    "# If you want to change it use the below.\n",
    "# plt.rcParams['figure.figsize'] = [15.0, 10.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Configs\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_kaggle():\n",
    "    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "def line(width=40):\n",
    "    print('-' * width)\n",
    "    \n",
    "def data_info(df, sample_size=10):\n",
    "    # print(df.info())\n",
    "    display(df.describe(include='all'))\n",
    "    display(df.sample(sample_size))\n",
    "\n",
    "def print_null_columns(df):\n",
    "    print('Columns with null values:\\n', df.isnull().sum())\n",
    "    \n",
    "def plot_tree(model, estimator_index=0, feature_names=None, class_names=None):\n",
    "    '''\n",
    "    Plot model.\n",
    "    Usage: lot_tree(model, feature_names=X_train.columns, class_names=['Win', 'Draw', 'Lose'])\n",
    "    '''\n",
    "    estimator = model.estimators_[estimator_index]\n",
    "    dot_data = tree.export_graphviz(\n",
    "        estimator, out_file=None, feature_names=feature_names,\n",
    "        class_names=class_names, filled=True, rounded=True)\n",
    "    graph = graphviz.Source(dot_data) \n",
    "    display(graph)\n",
    "\n",
    "def plot_xgb(model):\n",
    "    '''\n",
    "    Plot XGBClassifier.\n",
    "    Usage: plot_xgb(model)\n",
    "    '''\n",
    "    display(xgb.plot_importance(model))\n",
    "    display(xgb.to_graphviz(model, rankdir='LR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "db_path = Path('/kaggle/input/soccer/database.sqlite' if is_kaggle() else 'data/database.sqlite')\n",
    "conn = sqlite3.connect(str(db_path))\n",
    "n_jobs = 1\n",
    "\n",
    "player_data = pd.read_sql('SELECT * FROM player;', conn)\n",
    "player_stats_data = pd.read_sql('SELECT * FROM Player_Attributes;', conn)\n",
    "team_data = pd.read_sql('SELECT * FROM Team;', conn)\n",
    "match_data = pd.read_sql('SELECT * FROM Match;', conn)\n",
    "\n",
    "ROWS = [\"country_id\", \"league_id\", \"season\", \"stage\", \"date\", \"match_api_id\", \"home_team_api_id\", \n",
    "        \"away_team_api_id\", \"home_team_goal\", \"away_team_goal\", \"home_player_1\", \"home_player_2\",\n",
    "        \"home_player_3\", \"home_player_4\", \"home_player_5\", \"home_player_6\", \"home_player_7\", \n",
    "        \"home_player_8\", \"home_player_9\", \"home_player_10\", \"home_player_11\", \"away_player_1\",\n",
    "        \"away_player_2\", \"away_player_3\", \"away_player_4\", \"away_player_5\", \"away_player_6\",\n",
    "        \"away_player_7\", \"away_player_8\", \"away_player_9\", \"away_player_10\", \"away_player_11\"]\n",
    "\n",
    "match_data.dropna(subset=ROWS, inplace=True)\n",
    "match_data = match_data.tail(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info(player_stats_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info(team_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info(player_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info(match_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_label(match):\n",
    "    '''Derives a label for a given match.'''\n",
    "    home_goals = match['home_team_goal']\n",
    "    away_goals = match['away_team_goal']\n",
    "\n",
    "    label = pd.DataFrame()\n",
    "    label.loc[0, 'match_api_id'] = match['match_api_id']\n",
    "\n",
    "    if home_goals > away_goals:\n",
    "        label.loc[0, 'label'] = 'Win'\n",
    "    if home_goals == away_goals:\n",
    "        label.loc[0, 'label'] = 'Draw'\n",
    "    if home_goals < away_goals:\n",
    "        label.loc[0, 'label'] = 'Defeat'\n",
    "\n",
    "    return label.loc[0]\n",
    "\n",
    "def get_fifa_stats(match, player_stats):\n",
    "    '''Aggregate FIFA stats for a given match.'''\n",
    "    match_id = match.match_api_id\n",
    "    date = match['date']\n",
    "    players = ['home_player_1', 'home_player_2', 'home_player_3', \"home_player_4\", \"home_player_5\",\n",
    "               \"home_player_6\", \"home_player_7\", \"home_player_8\", \"home_player_9\", \"home_player_10\",\n",
    "               \"home_player_11\", \"away_player_1\", \"away_player_2\", \"away_player_3\", \"away_player_4\",\n",
    "               \"away_player_5\", \"away_player_6\", \"away_player_7\", \"away_player_8\", \"away_player_9\",\n",
    "               \"away_player_10\", \"away_player_11\"]\n",
    "    player_stats_new = pd.DataFrame()\n",
    "    names = []\n",
    "\n",
    "    for player in players:\n",
    "        player_id = match[player]\n",
    "        stats = player_stats[player_stats.player_api_id == player_id]\n",
    "        current_stats = stats[stats.date < date].sort_values(by='date', ascending=False)[:1]\n",
    "\n",
    "        if np.isnan(player_id):\n",
    "            # NOTE: Defaulting to the first item?\n",
    "            overall_rating = pd.Series(0)\n",
    "        else:\n",
    "            current_stats.reset_index(inplace=True, drop=True)\n",
    "            overall_rating = pd.Series(current_stats.loc[0, 'overall_rating'])\n",
    "\n",
    "        name = '{}_overall_rating'.format(player)\n",
    "        names.append(name)\n",
    "\n",
    "        player_stats_new = pd.concat([player_stats_new, overall_rating], axis=1)\n",
    "\n",
    "    player_stats_new.columns = names\n",
    "    player_stats_new['match_api_id'] = match_id\n",
    "\n",
    "    player_stats_new.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return player_stats_new.iloc[0]\n",
    "\n",
    "def get_fifa_data(matches, player_stats, path=None, data_exists=False):\n",
    "    '''Gets FIFA data for all matches.'''\n",
    "    if data_exists:\n",
    "        fifa_data = pd.read_pickle(path)\n",
    "    else:\n",
    "        print('Collecting FIFA data for each match...')\n",
    "        start = time()\n",
    "        fifa_data = matches.apply(lambda x: get_fifa_stats(x, player_stats), axis=1)\n",
    "        end = time()\n",
    "        print('FIFA data collected in {:.1f} minutes'.format((end - start)/60))\n",
    "\n",
    "    return fifa_data\n",
    "\n",
    "def get_overall_fifa_rankings(fifa, get_overall=False):\n",
    "    '''Get overall FIFA rankings from FIFA data.'''\n",
    "    temp_data = fifa\n",
    "\n",
    "    if get_overall:\n",
    "        data = temp_data.loc[:, fifa.columns.str.contains('overall_rating')]\n",
    "        data.loc[:, 'match_api_id'] = temp_data.loc[:, 'match_api_id']\n",
    "    else:\n",
    "        cols = fifa.loc[:, fifa.columns.str.contains('date_stat')]\n",
    "        temp_data = fifa.drop(cols.columns, axis=1)\n",
    "        data = temp_data\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_last_matches(matches, date, team, x=10):\n",
    "    '''Get the last matches of a given team.'''\n",
    "    team_matches = matches[\n",
    "        (matches['home_team_api_id'] == team) | (matches['away_team_api_id'] == team)\n",
    "    ]\n",
    "    last_matches = team_matches[team_matches.date < date].sort_values(by='date', ascending=False).iloc[0:x, :]\n",
    "    return last_matches\n",
    "\n",
    "def get_last_matches_against_each_other(matches, date, home_team, away_team, x=10):\n",
    "    '''Get the last x matches of two given teams.'''\n",
    "    home_matches = matches[(matches['home_team_api_id'] == home_team) &\n",
    "                           (matches['away_team_api_id'] == away_team)]\n",
    "    away_matches = matches[(matches['home_team_api_id'] == away_team) &\n",
    "                           (matches['away_team_api_id'] == home_team)]\n",
    "    total_matches = pd.concat([home_matches, away_matches])\n",
    "\n",
    "    min_x = min(x, total_matches.shape[0])\n",
    "    matches_before = total_matches[total_matches.date < date]\n",
    "    matches_sorted = matches_before.sort_values(by='date', ascending=False)\n",
    "    return matches_sorted.iloc[0:min_x, :]\n",
    "\n",
    "def get_goals(matches, team):\n",
    "    '''Get the goals of a specific team frm a set of matches.'''\n",
    "    home_goals = int(matches.home_team_goal[matches.home_team_api_id == team].sum())\n",
    "    away_goals = int(matches.away_team_goal[matches.away_team_api_id == team].sum())\n",
    "\n",
    "    total_goals = home_goals + away_goals\n",
    "    return total_goals\n",
    "\n",
    "def get_goals_conceded(matches, team):\n",
    "    '''Get the goals conceded of a specific team from a set of matches.'''\n",
    "    home_goals = int(matches.home_team_goal[matches.away_team_api_id == team].sum())\n",
    "    away_goals = int(matches.away_team_goal[matches.home_team_api_id == team].sum())\n",
    "\n",
    "    total_goals = home_goals + away_goals\n",
    "    return total_goals\n",
    "\n",
    "def get_wins(matches, team):\n",
    "    '''Get the number of wins of a specific team from a set of matches.'''\n",
    "    home_wins = int(matches.home_team_goal[\n",
    "        (matches.home_team_api_id == team) &\n",
    "        (matches.home_team_goal > matches.away_team_goal)\n",
    "    ].count())\n",
    "    away_wins = int(matches.away_team_goal[\n",
    "        (matches.away_team_api_id == team) &\n",
    "        (matches.away_team_goal > matches.home_team_goal)\n",
    "    ].count())\n",
    "    total_wins = home_wins + away_wins\n",
    "    return total_wins\n",
    "\n",
    "def get_match_features(match, matches, x=10):\n",
    "    '''Create match specific features for a given match.'''\n",
    "    date = match.date\n",
    "    home_team = match.home_team_api_id\n",
    "    away_team = match.away_team_api_id\n",
    "\n",
    "    matches_home_team = get_last_matches(matches, date, home_team, x)\n",
    "    matches_away_team = get_last_matches(matches, date, away_team, x)\n",
    "\n",
    "    last_matches_against = get_last_matches_against_each_other(\n",
    "        matches, date, home_team, away_team, x # =3\n",
    "    )\n",
    "\n",
    "    home_goals = get_goals(matches_home_team, home_team)\n",
    "    away_goals = get_goals(matches_away_team, away_team)\n",
    "    home_goals_conceded = get_goals_conceded(matches_home_team, home_team)\n",
    "    away_goals_conceded = get_goals_conceded(matches_away_team, away_team)\n",
    "\n",
    "    result = pd.DataFrame()\n",
    "    result.loc[0, 'match_api_id'] = match.match_api_id\n",
    "    result.loc[0, 'league_id'] = match.league_id\n",
    "    result.loc[0, 'home_team_goals_difference'] = home_goals - home_goals_conceded\n",
    "    result.loc[0, 'away_team_goals_difference'] = away_goals - away_goals_conceded\n",
    "    result.loc[0, 'games_won_home_team'] = get_wins(matches_home_team, home_team)\n",
    "    result.loc[0, 'games_won_away_team'] = get_wins(matches_away_team, away_team)\n",
    "    result.loc[0, 'games_against_won'] = get_wins(last_matches_against, home_team)\n",
    "    result.loc[0, 'games_against_lost'] = get_wins(last_matches_against, away_team)\n",
    "\n",
    "    return result.loc[0]\n",
    "\n",
    "def create_feables(matches, fifa, bookkeepers, get_overall=False, horizontal=True, x=10, verbose=True):\n",
    "    '''Create and aggregate features and labels for all matches.'''\n",
    "\n",
    "    fifa_stats = get_overall_fifa_rankings(fifa, get_overall)\n",
    "\n",
    "    if verbose:\n",
    "        print('Generating match features...')\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    match_stats = matches.apply(lambda x: get_match_features(x, matches, x=10), axis=1)\n",
    "\n",
    "    dummies = pd.get_dummies(match_stats['league_id']).rename(columns=lambda x: 'League_' + str(x))\n",
    "    match_stats = pd.concat([match_stats, dummies], axis=1)\n",
    "    match_stats.drop(['league_id'], inplace=True, axis=1)\n",
    "    end = time()\n",
    "\n",
    "    if verbose:\n",
    "        print('Match features generated in {:.1f} minutes'.format((end - start) / 60))\n",
    "\n",
    "    if verbose:\n",
    "        print('Generating match labels...')\n",
    "\n",
    "    start = time()\n",
    "    labels = matches.apply(get_match_label, axis=1)\n",
    "    end = time()\n",
    "\n",
    "    if verbose:\n",
    "        print('Match labels generated in {:.1f} minutes'.format((end - start) / 60))\n",
    "\n",
    "    if verbose:\n",
    "        print('Generating bookkeeper data...')\n",
    "\n",
    "    start = time()\n",
    "    bk_data = get_bookkeeper_data(matches, bookkeepers, horizontal=True)\n",
    "    bk_data.loc[:, 'match_api_id'] = matches.loc[:, 'match_api_id']\n",
    "    end = time()\n",
    "    if verbose:\n",
    "        print('Bookkeeper data generated in {:.1f} minutes'.format((end - start) / 60))\n",
    "\n",
    "    features = pd.merge(match_stats, fifa_stats, on='match_api_id', how='left')\n",
    "    features = pd.merge(features, bk_data, on='match_api_id', how='left')\n",
    "    feables = pd.merge(features, labels, on='match_api_id', how='left')\n",
    "\n",
    "    feables.dropna(inplace=True)\n",
    "\n",
    "    return feables\n",
    "\n",
    "def train_classifier(clf, dm_reduction, X_train, y_train, cv_sets, params, scorer,\n",
    "                     jobs, use_grid_search=True, best_components=None, best_params=None):\n",
    "    '''Fits a classifier to the training data.'''\n",
    "    start = time()\n",
    "    if use_grid_search:\n",
    "        estimators = [('dm_reduce', dm_reduction), ('clf', clf)]\n",
    "        pipeline = Pipeline(estimators)\n",
    "\n",
    "        grid_obj = model_selection.GridSearchCV(pipeline, param_grid=params, scoring=scorer, cv=cv_sets, n_jobs=jobs)\n",
    "        grid_obj.fit(X_train, y_train)\n",
    "        best_pipe = grid_obj.best_estimator_\n",
    "    else:\n",
    "        estimators = [('dm_reduce', dm_reduction(n_components=best_components)), ('clf', clf(best_params))]\n",
    "        pipeline = Pipeline(estimators)\n",
    "        best_pipe = pipeline.fit(X_train, y_train)\n",
    "\n",
    "    end = time()\n",
    "\n",
    "    print('Trained {} in {:.1f} minutes'.format(clf.__class__.__name__, (end - start) / 60))\n",
    "\n",
    "    return best_pipe\n",
    "\n",
    "def predict_labels(clf, best_pipe, features, target):\n",
    "    '''Make predictions using a fit classifier based on scorer.'''\n",
    "    start = time()\n",
    "    y_pred = clf.predict(best_pipe.named_steps['dm_reduce'].transform(features))\n",
    "    end = time()\n",
    "\n",
    "    print('Made predictions in {:.4f} seconds'.format(end - start))\n",
    "    return accuracy_score(target.values, y_pred)\n",
    "\n",
    "def train_calibrate_predict(clf, dm_reduction, X_train, y_train, X_calibrate, y_calibrate,\n",
    "                            X_test, y_test, cv_sets, params, scorer, jobs, use_grid_search=True, **kwargs):\n",
    "    '''Train and predict using a classifier based on scorer.'''\n",
    "    print('----------{}----------'.format(clf.__class__.__name__))\n",
    "    print('Training a {} with {}...'.format(clf.__class__.__name__,\n",
    "                                            dm_reduction.__class__.__name__))\n",
    "    best_pipe = train_classifier(clf, dm_reduction, X_train, y_train, cv_sets, params, scorer, jobs)\n",
    "\n",
    "    print('Calibrating probabilities of classifier...')\n",
    "    start = time()\n",
    "    # 'isotonic' fails for XGBoost with \"ValueError: Buffer dtype mismatch, expected 'float' but got 'double'\"\n",
    "    # https://github.com/scikit-learn/scikit-learn/pull/14902\n",
    "    clf = CalibratedClassifierCV(best_pipe.named_steps['clf'], cv='prefit', method='sigmoid')\n",
    "    clf.fit(best_pipe.named_steps['dm_reduce'].transform(X_calibrate), y_calibrate)\n",
    "    end = time()\n",
    "    print('Calibrated {} in {:.1f} minutes'.format(clf.__class__.__name__, (end - start) / 60))\n",
    "\n",
    "    print('Score of {} for training set: {:.4f}.'.format(\n",
    "        clf.__class__.__name__, predict_labels(clf, best_pipe, X_train, y_train)))\n",
    "    print('Score of {} for test set: {:.4f}.'.format(\n",
    "        clf.__class__.__name__, predict_labels(clf, best_pipe, X_test, y_test)))\n",
    "\n",
    "    return clf, best_pipe.named_steps['dm_reduce'], predict_labels(\n",
    "        clf, best_pipe, X_train, y_train), predict_labels(clf, best_pipe, X_test, y_test)\n",
    "\n",
    "def convert_odds_to_prob(match_odds):\n",
    "    '''Converts bookkeeper odds to probabilities.'''\n",
    "    match_id = match_odds.loc[:, 'match_api_id']\n",
    "    bookkeeper = match_odds.loc[:, 'bookkeeper']\n",
    "    win_odd = match_odds.loc[:, 'Win']\n",
    "    draw_odd = match_odds.loc[:, 'Draw']\n",
    "    loss_odd = match_odds.loc[:, 'Defeat']\n",
    "\n",
    "    win_prob = 1 / win_odd\n",
    "    draw_prob = 1 / draw_odd\n",
    "    loss_prob = 1 / loss_odd\n",
    "\n",
    "    total_prob = win_prob + draw_prob + loss_prob\n",
    "\n",
    "    probs = pd.DataFrame()\n",
    "    probs.loc[:, 'match_api_id'] = match_id\n",
    "    probs.loc[:, 'bookkeeper'] = bookkeeper\n",
    "    probs.loc[:, 'Win'] = win_prob / total_prob\n",
    "    probs.loc[:, 'Draw'] = draw_prob / total_prob\n",
    "    probs.loc[:, 'Defeat'] = loss_prob / total_prob\n",
    "\n",
    "    return probs\n",
    "\n",
    "# bookkeepers: Bookkeeper tag\n",
    "def get_bookkeeper_data(matches, bookkeepers, horizontal=True):\n",
    "    '''Aggregates bookkeeper data for all matches and bookkeepers'''\n",
    "    bk_data = pd.DataFrame()\n",
    "\n",
    "    for bookkeeper in bookkeepers:\n",
    "        temp_data = matches.loc[:, matches.columns.str.contains(bookkeeper)]\n",
    "        temp_data.loc[:, 'bookkeeper'] = str(bookkeeper)\n",
    "        temp_data.loc[:, 'match_api_id'] = matches.loc[:, 'match_api_id']\n",
    "\n",
    "        cols = temp_data.columns.values\n",
    "        cols[:3] = ['Win', 'Draw', 'Defeat']\n",
    "        temp_data.columns = cols\n",
    "        temp_data.loc[:, 'Win'] = pd.to_numeric(temp_data['Win'])\n",
    "        temp_data.loc[:, 'Draw'] = pd.to_numeric(temp_data['Draw'])\n",
    "        temp_data.loc[:, 'Defeat'] = pd.to_numeric(temp_data['Defeat'])\n",
    "\n",
    "        if horizontal:\n",
    "            temp_data = convert_odds_to_prob(temp_data)\n",
    "            temp_data.drop('match_api_id', axis=1, inplace=True)\n",
    "            temp_data.drop('bookkeeper', axis=1, inplace=True)\n",
    "\n",
    "            win_name = bookkeeper + '_Win'\n",
    "            draw_name = bookkeeper + '_Draw'\n",
    "            defeat_name = bookkeeper + '_Defeat'\n",
    "            temp_data.columns.values[:3] = [win_name, draw_name, defeat_name]\n",
    "\n",
    "            bk_data = pd.concat([bk_data, temp_data], axis=1)\n",
    "        else:\n",
    "            bk_data = bk_data.append(temp_data, ignore_index=True)\n",
    "\n",
    "    if horizontal:\n",
    "        temp_data.loc[:, 'match_api_id'] = matches.loc[:, 'match_api_id']\n",
    "\n",
    "    return bk_data\n",
    "\n",
    "def get_bookkeeper_probs(matches, bookkeepers, horizontal=False):\n",
    "    '''Get bookkeeper data and convert to probabilities for vertical aggregation.'''\n",
    "    data = get_bookkeeper_data(matches, bookkeepers, horizontal=False)\n",
    "    probs = convert_odds_to_prob(data)\n",
    "    return probs\n",
    "\n",
    "def plot_confusion_matrix(y_test, X_test, clf, dim_reduce, path, cmap=plt.cm.Blues, normalize=False):\n",
    "    '''Plot confusion matrix for given classifier and data.'''\n",
    "    labels = ['Win', 'Draw', 'Defeat']\n",
    "    cm = confusion_matrix(y_test, clf.predict(dim_reduce.transform(X_test)), labels)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum()\n",
    "\n",
    "    sns.set_style('whitegrid', {'axes.grid': False})\n",
    "    fig = plt.figure(1)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    title = 'Confusion matrix of a {} with {}'.format(\n",
    "        best_clf.base_estimator.__class__.__name__,\n",
    "        best_dm_reduce.__class__.__name__\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j], 2),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    y_pred = clf.predict(dim_reduce.transform(X_test))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "def explore_data(features, inputs, path):\n",
    "    '''Explore data by plotting KDE graphs.'''\n",
    "    fig = plt.figure(1)\n",
    "    fig.subplots_adjust(bottom=-1, left=0.025, top=2, right=0.975)\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(10)\n",
    "\n",
    "    i = 1\n",
    "    for col in features.columns:\n",
    "        sns.set_style('whitegrid')\n",
    "        sns.set_context('paper', rc={'lines.linewidth': 1})\n",
    "        plt.subplot(10, 5, i)\n",
    "        j = i - 1\n",
    "\n",
    "        sns.distplot(inputs[inputs['label'] == 'Win'].iloc[:, j], hist=False, label='Win')\n",
    "        sns.distplot(inputs[inputs['label'] == 'Draw'].iloc[:, j], hist=False, label='Draw')\n",
    "        sns.distplot(inputs[inputs['label'] == 'Defeat'].iloc[:, j], hist=False, label='Defeat')\n",
    "        plt.legend()\n",
    "        i = i + 1\n",
    "\n",
    "    DefaultSize = fig.get_size_inches()\n",
    "    fig.set_size_inches((DefaultSize[0] * 1.2, DefaultSize[1] * 1.2))\n",
    "    plt.show()\n",
    "\n",
    "    labels = inputs.loc[:, 'label']\n",
    "    class_weights = labels.value_counts() / len(labels)\n",
    "    print(class_weights)\n",
    "\n",
    "    feature_details = features.describe().transpose()\n",
    "\n",
    "    return feature_details\n",
    "\n",
    "def find_best_classifier(classifiers, dm_reductions, scorer, X_t, y_t, X_c, y_c,\n",
    "                         X_v, y_v, cv_sets, params, jobs):\n",
    "    '''Tune all classifier and dimensionality reduction combinations to find best classifier.'''\n",
    "    clfs_return = []\n",
    "    dm_reduce_return = []\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "\n",
    "    for dm in dm_reductions:\n",
    "        for clf in classifiers:\n",
    "            clf, dm_reduce, train_score, test_score = train_calibrate_predict(\n",
    "                clf=clf, dm_reduction=dm, X_train=X_t, y_train=y_t, X_calibrate=X_c,\n",
    "                y_calibrate=y_c, X_test=X_v, y_test=y_v, cv_sets=cv_sets,\n",
    "                params=params[clf], scorer=scorer, jobs=jobs, use_grid_search=True)\n",
    "\n",
    "            clfs_return.append(clf)\n",
    "            dm_reduce_return.append(dm_reduce)\n",
    "            train_scores.append(train_score)\n",
    "            test_scores.append(test_score)\n",
    "\n",
    "        return clfs_return, dm_reduce_return, train_scores, test_scores\n",
    "\n",
    "def plot_training_results(classifiers, dm_reductions, train_scores, test_scores, path):\n",
    "    '''Plot results of classifier training.'''\n",
    "    sns.set_style('whitegrid')\n",
    "    sns.set_context('paper', font_scale=1, rc={'lines.linewidth': 1})\n",
    "    ax = plt.subplot(111)\n",
    "    w = 0.5\n",
    "    x = np.arange(len(train_scores))\n",
    "    ax.set_yticks(x + w)\n",
    "    ax.legend ((train_scores[0], test_scores[0]), ('Train Scores', 'Test Socores'))\n",
    "    names = []\n",
    "\n",
    "    for i in range(0, len(classifiers)):\n",
    "        clf = classifiers[i]\n",
    "        clf_name = clf.base_estimator.__class__.__name__\n",
    "        dm = dm_reductions[i]\n",
    "        dm_name = dm.__class__.__name__\n",
    "\n",
    "        name = '{} with {}'.format(clf_name, dm_name)\n",
    "        names.append(name)\n",
    "\n",
    "    ax.set_yticklabels((names))\n",
    "    plt.xlim(0.4, 0.6)\n",
    "    plt.barh(x, test_scores, color='b', alpha=0.6)\n",
    "    plt.title('Test Data Accuracy Scores')\n",
    "    fig = plt.figure(1)\n",
    "    plt.show()\n",
    "\n",
    "def plot_bookkeeper_cf_matrix(matches, bookkeepers, path, verbose=False, normalize=True):\n",
    "    '''Plot confusion matrix of bookkeeper predictions.'''\n",
    "    if verbose:\n",
    "        print('Obtaining labels...')\n",
    "\n",
    "    y_test_temp = matches.apply(get_match_label, axis=1)\n",
    "\n",
    "    if verbose:\n",
    "        print('Obtaining bookkeeper probabilities...')\n",
    "\n",
    "    bookkeeper_probs = get_bookkeeper_probs(matches, bookkeepers)\n",
    "    bookkeeper_probs.reset_index(inplace=True, drop=True)\n",
    "    bookkeeper_probs.dropna(inplace=True)\n",
    "\n",
    "    if verbose:\n",
    "        print('Obtaining bookkeeper labels...')\n",
    "\n",
    "    y_pred_temp = pd.DataFrame()\n",
    "    y_pred_temp.loc[:, 'bk_label'] = bookkeeper_probs[['Win', 'Draw', 'Defeat']].idxmax(axis=1)\n",
    "    y_pred_temp.loc[:, 'match_api_id'] = bookkeeper_probs.loc[:, 'match_api_id']\n",
    "\n",
    "    if verbose:\n",
    "        print('Plotting confusion matrix...')\n",
    "\n",
    "    results = pd.merge(y_pred_temp, y_test_temp, on='match_api_id', how='left')\n",
    "    y_test = results.loc[:, 'label']\n",
    "    y_pred = results.loc[:, 'bk_label']\n",
    "\n",
    "    labels = ['Win', 'Draw', 'Defeat']\n",
    "    cm = confusion_matrix(y_test, y_pred, labels)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum()\n",
    "\n",
    "    sns.set_style('whitegrid', {'axes.grid': False})\n",
    "    fig = plt.figure(1)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    title = 'Confusion matrix of Bookkeeper predictions!'\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j], 2),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('Bookkeeper score for test set: {:.4f}.'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate FIFA data\n",
    "fifa_data = get_fifa_data(match_data, player_stats_data, data_exists=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info(fifa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and labels.\n",
    "bk_cols = ['B365', 'BW', 'IW', 'LB', 'PS', 'WH', 'SJ', 'VC', 'GB', 'BS']\n",
    "bk_cols_selected = ['B365', 'BW']\n",
    "feables = create_feables(match_data, fifa_data, bk_cols_selected, get_overall=True)\n",
    "inputs = feables.drop('match_api_id', axis=1)\n",
    "\n",
    "# Explore data and create visualization.\n",
    "labels = inputs.loc[:, 'label']\n",
    "features = inputs.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_details = explore_data(features, inputs, db_path)\n",
    "\n",
    "# Split the data into train, calibrate and test data sets.\n",
    "X_train_calibrate, X_test, y_train_calibrate, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "X_train, X_calibrate, y_train, y_calibrate = train_test_split(\n",
    "    X_train_calibrate, y_train_calibrate, test_size=0.3, random_state=42,\n",
    "    stratify=y_train_calibrate)\n",
    "\n",
    "# Create cross validation data splits.\n",
    "cv_sets = model_selection.StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=5)\n",
    "cv_sets.get_n_splits(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models and parameters\n",
    "\n",
    "# Initialize classifiers\n",
    "RF_clf = RandomForestClassifier(n_estimators=200, random_state=1, class_weight='balanced')\n",
    "AB_clf = AdaBoostClassifier(n_estimators=200, random_state=2)\n",
    "GNB_clf = GaussianNB()\n",
    "KNN_clf = KNeighborsClassifier()\n",
    "LOG_clf = linear_model.LogisticRegression(multi_class='ovr', solver='sag', class_weight='balanced')\n",
    "XG_clf = xgb.XGBClassifier(n_estimators=200, random_state=3)\n",
    "clfs = [RF_clf, AB_clf, GNB_clf, KNN_clf, LOG_clf, XG_clf]\n",
    "\n",
    "# Grid search parameters\n",
    "feature_len = features.shape[1]\n",
    "scorer = make_scorer(accuracy_score)\n",
    "dm_comp_count = np.arange(5, feature_len, np.around(feature_len / 5)).astype('int')\n",
    "parameters_RF = {'clf__max_features': ['auto', 'log2'],\n",
    "                 'dm_reduce__n_components': dm_comp_count}\n",
    "parameters_AB = {'clf__learning_rate': np.linspace(0.5, 2, 5),\n",
    "                 'dm_reduce__n_components': dm_comp_count}\n",
    "parameters_GNB = {'dm_reduce__n_components': dm_comp_count}\n",
    "parameters_KNN = {'clf__n_neighbors': [3, 5, 10],\n",
    "                  'dm_reduce__n_components': dm_comp_count}\n",
    "parameters_LOG = {'clf__C': np.logspace(1, 1000, 5),\n",
    "                  'dm_reduce__n_components': dm_comp_count}\n",
    "parameters_XG = {'clf__max_depth': [10, 50, 100],\n",
    "                 'dm_reduce__n_components': dm_comp_count}\n",
    "parameters = {clfs[0]: parameters_RF,\n",
    "              clfs[1]: parameters_AB,\n",
    "              clfs[2]: parameters_GNB,\n",
    "              clfs[3]: parameters_KNN,\n",
    "              clfs[4]: parameters_LOG,\n",
    "              clfs[5]: parameters_XG}\n",
    "\n",
    "# Dimensionality reduction\n",
    "pca = PCA()\n",
    "dm_reductions = [pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a baseline model (GBC)\n",
    "\n",
    "clf = LOG_clf\n",
    "clf.fit(X_train, y_train)\n",
    "print('Score of {} for training set: {:.4f}.'.format(\n",
    "    clf.__class__.__name__, accuracy_score(y_train, clf.predict(X_train))))\n",
    "print('Score of {} for test set: {:.4f}.'.format(\n",
    "    clf.__class__.__name__, accuracy_score(y_test, clf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best classifier\n",
    "\n",
    "clfs, dm_reductions, train_scores, test_scores = find_best_classifier(\n",
    "    clfs, dm_reductions, scorer, X_train, y_train, X_calibrate, y_calibrate,\n",
    "    X_test, y_test, cv_sets, parameters, n_jobs)\n",
    "\n",
    "plot_training_results(clfs, dm_reductions, np.array(train_scores),\n",
    "                      np.array(test_scores), path=db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix of the best model and the bookkeeper predictions\n",
    "\n",
    "best_clf = clfs[np.argmax(test_scores)]\n",
    "best_dm_reduce = dm_reductions[np.argmax(test_scores)]\n",
    "print('The best classifier is a {} with {}.'.format(\n",
    "    best_clf.base_estimator.__class__.__name__, best_dm_reduce.__class__.__name__))\n",
    "plot_confusion_matrix(y_test, X_test, best_clf, best_dm_reduce, path=db_path, normalize=True)\n",
    "\n",
    "plot_bookkeeper_cf_matrix(match_data, bk_cols, db_path, verbose=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time()\n",
    "print('Program run in {:.1f} minutes'.format((end - start) / 60))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
